{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Challenges in Linear Regression\n",
    "\n",
    "A dataset may face several issues which could make it unsuitable to fit a linear model on it. Some of these issues can be mitigated, while others may require that you rethink your approach.\n",
    "\n",
    "We will demonstrate problems with scaling, multicollinearity, heteroscedasticity, overfitting, and more. To do this, we will use different datasets, as, if a single dataset faces all these challenges, you may be better off using another model.\n",
    "\n",
    "Let's start by loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np; import matplotlib.pyplot as plt; from matplotlib.ticker import MaxNLocator; import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Interpretability\n",
    "We cannot compare coefficients in linear regression and make claims about which predictors are stronger or weaker unless they are on the same scale. So, rescaling the predictors to a common scale can enable analysis of feature importance.\n",
    "\n",
    "We will use a data which contains information about the personalities of various people, with the target being the size of their friends circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personality = pd.read_csv('personality_data.csv'); df_personality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personality = df_personality[['Alone Time Hours', 'Social Events', 'Personality', 'Friends Circle Size']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to deal with the categorical predictor `'Personality'` and turn it into numeric type. As this is just a binary class, doing this is fairly straightforward: we will assign `'Introvert'` to $0$ and `'Extrovert'` to $1$. Thus, this predictor is now an indicator of extroversion.\n",
    "\n",
    "The number of hours that a person spends alone, the number of social events they attend, and their personality are all on different scales. We can fit a linear model to predict their friends circle size without scaling to make interpretations from the coefficients, but we will not be able to fairly compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personality['Extrovert'] = df_personality['Personality'].replace({'Introvert': 0, 'Extrovert': 1})  # One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_personality[['Alone Time Hours', 'Social Events', 'Extrovert']]  # Predictors\n",
    "y = df_personality['Friends Circle Size']  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fit a linear regression model using the unscaled features and examine the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit linear regression without scaling\n",
    "model_unscaled = LinearRegression()\n",
    "model_unscaled.fit(X, y)\n",
    "coef_df_unscaled = pd.DataFrame({'Feature': X.columns, 'Coefficient': model_unscaled.coef_}); coef_df_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it fair to compare the above coefficients if the predictors themselves are not on the same scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and model performance in unscaled case\n",
    "y_pred_unscaled = model_unscaled.predict(X)\n",
    "r2_unscaled = r2_score(y, y_pred_unscaled)\n",
    "mse_unscaled = mean_squared_error(y, y_pred_unscaled)\n",
    "print(f'R² Score: {r2_unscaled:.4f}'); print(f'MSE: {mse_unscaled:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit a linear regression model for the same data, but scale all predictors to the interval $[0, 1]$ using **min-max scaling**. Here, the minimum value of a predictor in our training set is represented by $0$ while the maximum is represented by $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardisation\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns = X.columns); X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the predictors are on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression with scaled features\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_scaled, y)\n",
    "coef_df_scaled = pd.DataFrame({'Feature': X_scaled.columns, 'Coefficient': model_scaled.coef_}); coef_df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for extroversion did not change because it was already in the $[0, 1]$ interval. Note how the coefficient for `'Alone Time Hours'` and `'Social Events'` have changed.\n",
    "\n",
    "We can now fairly compare the features using their coefficients. We can see that in terms of importance of the features, `'Extrovert'` $>$ `'Alone Time Hours'` $>$ `'Social Events'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and model performance in scaled case\n",
    "y_pred_scaled = model_scaled.predict(X_scaled)\n",
    "r2_scaled = r2_score(y, y_pred_scaled)\n",
    "mse_scaled = mean_squared_error(y, y_pred_scaled)\n",
    "print(f'R² Score: {r2_scaled:.4f}'); print(f'MSE: {mse_scaled:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that scaling did not do much for performance. This is expected, as scaling only improves the interpretability of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "Including too many predictors in a regression model can lead to redundancy among them, resulting in **multicollinearity**. We need diagnostics and techniques like adjusted $R^2$, the variance inflation factor or VIF, and recursive feature elimination or RFE to identify and address these issues.\n",
    "\n",
    "We'll use the California Housing dataset which is preloaded into `sklearn` for this demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset which is preloaded into scikit-learn\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns = housing.feature_names)  # Predictors\n",
    "y = housing.target  # The sale price of the house\n",
    "print('Shape of original data =', (X.shape[0], X.shape[1] + 1))  # Includes the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a smaller subset of the predictors and a random sample of the entries\n",
    "X = X[['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']].sample(100, random_state = 90)\n",
    "y = y[X.index]  # Ensure that the entries in y are the same as the corresponding ones in X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of subset data =', (X.shape[0], X.shape[1] + 1))  # Includes the target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more predictors in a regression model are highly correlated, meaning they provide redundant information about the target's variation. This raises various issues. For instance, small changes in data can cause large changes in coefficient estimates and it can get difficult to isolate the individual effect of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix and heatmap\n",
    "plt.figure(figsize = (6, 4))\n",
    "sns.heatmap(X.select_dtypes(include = ['number']).corr().round(2), annot = True, cmap = 'RdBu_r', center = 0, cbar_kws = {'label': 'Correlation Coefficient'})\n",
    "plt.title('Correlation Matrix'); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `'AveRooms'` and `'AveBedrms'` seem highly correlated. Yet, we will proceed to fitting `LinearRegression()` on all the predictors to study the impact of the inclusion of correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = LinearRegression()\n",
    "model_all.fit(X, y)\n",
    "coeff_df = pd.DataFrame({'Feature': X.columns.tolist() + ['Intercept'], 'Coefficient': model_all.coef_.tolist() + [model_all.intercept_]})\n",
    "display(coeff_df); print('R² =', round(model_all.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Features Based on the Sum of Absolute Correlations\n",
    "\n",
    "Let's now try to remove predictors based on multicollinearity. For this, let's develop a simple heuristic. For each predictor, we'll calculate the sum of the absolute correlation values with all other predictors, and remove the feature with the highest sum. We'll repeat this process until there is just one feature left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_feature_remover(inputdf, target):\n",
    "    df = inputdf.copy()\n",
    "    \n",
    "    # Initial regression model with all predictors\n",
    "    model = LinearRegression()\n",
    "    model.fit(df, target)\n",
    "    \n",
    "    records = []  # To store iteration number, number of features at iteration, maximum correlation sum, feature to drop, and R-squared before dropping\n",
    "    iteration = 1  # Iteration number\n",
    "\n",
    "    while df.shape[1] > 1:\n",
    "        correlation_matrix = df.corr()\n",
    "        sum_abs_corr = correlation_matrix.abs().sum()  # No need to exclude self correlation as heuristic is relative\n",
    "        predictor_to_drop = sum_abs_corr.idxmax()\n",
    "        max_corr_sum = sum_abs_corr.max()\n",
    "        r2_before_drop = model.score(df, target)\n",
    "        records.append({'Iteration': iteration, 'Num_Features': df.shape[1], 'Feature_To_Drop': predictor_to_drop,\n",
    "                        'Max_Correlation_Sum': round(max_corr_sum, 3), 'R2_Before_Drop': round(r2_before_drop, 3)})\n",
    "    \n",
    "        df = df.drop(columns = [predictor_to_drop])\n",
    "        model.fit(df, target)\n",
    "        iteration += 1\n",
    "\n",
    "    # Final iteration with one feature left\n",
    "    r2_final = model.score(df, target)\n",
    "    records.append({'Iteration': iteration, 'Num_Features': df.shape[1], 'Feature_To_Drop': None,\n",
    "                    'Max_Correlation_Sum': 0.0, 'R2_Before_Drop': round(r2_final, 3)})\n",
    "        \n",
    "    return pd.DataFrame(records).set_index('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_feature_remover(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our model any good? Should we have stopped dropping features earlier in this process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the R-squared change\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(correlated_feature_remover(X,y)['Num_Features'], correlated_feature_remover(X,y)['R2_Before_Drop'], marker = 'o')\n",
    "plt.xlabel('Number of Predictors'); plt.ylabel('R²'); plt.title('R² as a Function of Predictors'); plt.gca().xaxis.set_major_locator(MaxNLocator(integer = True)); plt.gca().invert_xaxis(); plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how $R^2$ always drops as you remove predictors, but some predictors do not offer much in explaining the target feature. $R^2$ only gives an idea about the proportion of variance explained. It always increases with more predictors, even if they are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted $R^2$\n",
    "\n",
    "Adjusted $R^2$ ($R_{\\mathrm{adj}}^2$), on the other hand, penalises the presence of irrelevant predictors. If $n$ is the number of observations, $p$ is the number of predictors, $\\text{RSS}$ is the sum of squared residuals, and $\\text{TSS}$ is the total sum of squares, we can calculate $R_{\\mathrm{adj}}^2$ as\n",
    "\n",
    "$$R_{\\mathrm{adj}}^2 = 1 - \\frac{(1-R^2) (n-1)}{n-p-1}$$\n",
    "\n",
    "As there's no built-in function in `sklearn` to calculate the adjusted $R^2$, we will manually functionalise this formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r2(r2, n, p): return 1 - (1 - r2) * (n - 1) / (n - p - 1)  # Function to calculate adjusted R²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our feature remover to also compute the adjusted $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_feature_remover_with_adj_r2(inputdf, target):\n",
    "    df = inputdf.copy()\n",
    "    \n",
    "    # Initial regression model with all predictors\n",
    "    model = LinearRegression()\n",
    "    model.fit(df, target)\n",
    "    \n",
    "    records = []  # To store iteration number, number of features at iteration, maximum correlation sum, feature to drop, R-squared, and adjusted R-squared before dropping\n",
    "    iteration = 1  # Iteration number\n",
    "    n = len(target)  # Number of samples\n",
    "\n",
    "    while df.shape[1] > 1:\n",
    "        correlation_matrix = df.corr()\n",
    "        sum_abs_corr = correlation_matrix.abs().sum()  # No need to exclude self correlation as heuristic is relative\n",
    "        predictor_to_drop = sum_abs_corr.idxmax()\n",
    "        max_corr_sum = sum_abs_corr.max()\n",
    "        r2_before_drop = model.score(df, target)\n",
    "        adj_r2_before_drop = adjusted_r2(r2_before_drop, n, df.shape[1])\n",
    "        records.append({'Iteration': iteration, 'Num_Features': df.shape[1], 'Feature_To_Drop': predictor_to_drop,\n",
    "                        'Max_Correlation_Sum': round(max_corr_sum, 3), 'R2_Before_Drop': round(r2_before_drop, 3),\n",
    "                        'Adjusted_R2_Before_Drop': round(adj_r2_before_drop, 3)})\n",
    "    \n",
    "        df = df.drop(columns = [predictor_to_drop])\n",
    "        model.fit(df, target)\n",
    "        iteration += 1\n",
    "\n",
    "    # Final iteration with one feature left\n",
    "    r2_final = model.score(df, target)\n",
    "    adj_r2_final = adjusted_r2(r2_final, n, df.shape[1])\n",
    "    records.append({'Iteration': iteration, 'Num_Features': df.shape[1], 'Feature_To_Drop': None,\n",
    "                    'Max_Correlation_Sum': 0.0, 'R2_Before_Drop': round(r2_final, 3),\n",
    "                    'Adjusted_R2_Before_Drop': round(adj_r2_final, 3)})\n",
    "        \n",
    "    return pd.DataFrame(records).set_index('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_remover = correlated_feature_remover_with_adj_r2(X, y)\n",
    "df_feature_remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the R-squared and adjusted-R-squared change\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(df_feature_remover['Num_Features'], df_feature_remover['R2_Before_Drop'], marker = 'o', label = 'R²')\n",
    "plt.plot(df_feature_remover['Num_Features'], df_feature_remover['Adjusted_R2_Before_Drop'], marker = 'o', label = 'Adjusted R²')\n",
    "plt.xlabel('Number of Predictors'); plt.ylabel('R² and Adjusted R²'); plt.title('R² and Adjusted R² as a Function of Predictors'); plt.gca().xaxis.set_major_locator(MaxNLocator(integer = True)); plt.gca().invert_xaxis(); plt.legend(); plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While adjusted $R^2$ and $R^2$ follow similar trends, adjusted $R^2$ penalises the model for having more predictors, so the separation between the two metrics will be larger for larger number of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Inflation Factor\n",
    "The variance inflation factor (VIF) measures the redundant contribution to target variance prediction due to predictors. For predictor $X_i$, $\\mathrm{VIF}_i = \\frac{1}{1 - R^2_i}$, where $R^2_i$ is the $R^2$ from regressing $X_i$ on all other predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the VIF using the `variance_inflation_factor()` method from `statsmodels.stats.outliers_influence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for all predictors\n",
    "def calculate_vif(dataframe):\n",
    "    vif_data = []\n",
    "    for i in range(dataframe.shape[1]):\n",
    "        vif_value = variance_inflation_factor(dataframe.values, i)\n",
    "        vif_data.append({'Predictor': dataframe.columns[i], 'VIF': vif_value})\n",
    "    return pd.DataFrame(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for original dataset\n",
    "vif_df = calculate_vif(X)\n",
    "vif_df = vif_df.sort_values('VIF', ascending = False)\n",
    "vif_df.round(3).set_index('Predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the following guidelines are recommended when working with VIF values\n",
    "- $\\text{VIF} = 1$ → No correlation between this predictor and others\n",
    "- $1 < \\text{VIF} < 5$ → Very mildly correlated and generally acceptable\n",
    "- $5 \\leq \\text{VIF} < 10$ → Moderately correlated and indicates potential multicollinearity issues\n",
    "- $\\text{VIF} \\geq 10$ → Strongly correlated and may necessitate treatment of this predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise VIF values\n",
    "def plot_vif(df):\n",
    "    plt.figure(figsize = (6, 4))\n",
    "    colors = ['red' if vif > 10 else 'yellow' if vif > 5 else 'green' for vif in df['VIF']]; bars = plt.bar(df['Predictor'], df['VIF'], color = colors, alpha = 0.7); plt.axhline(y = 5, color = 'orange', linestyle = '--', label = 'VIF = 5 (Moderate threshold)'); plt.axhline(y = 10, color = 'red', linestyle = '--', label = 'VIF = 10 (High threshold)'); plt.xlabel(' '); plt.ylabel('VIF Value'); plt.title('Variance Inflation Factor by Predictor'); plt.xticks(rotation = 45); plt.legend(); plt.grid(True, alpha = 0.3); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vif(vif_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a predictor has a high VIF, it can be predicted with high accuracy from the other predictors. It is redundant, because most of its information is already contained in the other predictors.\n",
    "\n",
    "Let’s pick some of the high-VIF predictors and try to predict them using the remaining predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression with a predictor as a target feature and other features (except the actual target) as predictors \n",
    "def feature_r2(df, target_feature):\n",
    "    X = df.drop(columns = [target_feature])  # Remove one predictor\n",
    "    y = df[target_feature]  # Set it as the target    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    return r2_score(y, lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_features = ['AveRooms', 'AveBedrms', 'MedInc']  # Check R² for high-VIF features\n",
    "for f in high_vif_features: print(f'R² for {f} ~ other predictors: {feature_r2(X, f):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `'AveRooms'` and `'AveBedrms'` can easily be predicted from the other predictors (likely, they are predicting each-other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vif_reduced = X.drop('AveRooms', axis = 1)  # Remove the feature with highest VIF\n",
    "vif_df_reduced = calculate_vif(X_vif_reduced)  # Recalculate the VIFs of the remaining predictors\n",
    "vif_df_reduced = vif_df_reduced.sort_values('VIF', ascending = False)\n",
    "vif_df_reduced.round(3).set_index('Predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the VIF scores have greatly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vif(vif_df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "model_original = LinearRegression().fit(X, y)\n",
    "model_vif_reduced = LinearRegression().fit(X_vif_reduced, y)\n",
    "r2_original = model_original.score(X, y)\n",
    "r2_reduced = model_vif_reduced.score(X_vif_reduced, y)\n",
    "adj_r2_original = adjusted_r2(r2_original, len(X), X.shape[1])\n",
    "adj_r2_reduced = adjusted_r2(r2_reduced, len(X_vif_reduced), X_vif_reduced.shape[1])\n",
    "\n",
    "print('Model Performance Comparison'); print(f'Original model: R²: {r2_original:.4f}, Adj R²: {adj_r2_original:.4f}'); print(f'VIF-reduced model: - R²: {r2_reduced:.4f}, Adj R²: {adj_r2_reduced:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drops in $R^2$ and adjusted $R^2$ are expected, but we need to take a call on whether we want to retain a highly correlated predictor or not based on business and domain knowledge and also considering factors such as training time and model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "Recursive feature elimination (RFE) is a systematic approach to feature selection that iteratively fits a model with features, ranks the features by importance, eliminates the least important feature, and repeats until a fixed number of features remain\n",
    "\n",
    "Let's return to our original data with all features intact and apply RFE to remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "rfe = RFE(estimator = model, n_features_to_select = 5)  # RFE with LR\n",
    "rfe.fit(X, y)\n",
    "selected = pd.DataFrame({'Feature': X.columns, 'Selected': rfe.support_, 'Coefficient': model.coef_.round(3)}); selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Population` was discarded by RFE because its contribution to predicting the target is weak compared to other features. Let's compare how the RFE model compares with the non-RFE one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rfe = X.loc[:, rfe.support_]\n",
    "model.fit(X_rfe, y)\n",
    "y_pred = model.predict(X_rfe)\n",
    "r2_rfe = r2_score(y, y_pred)\n",
    "print('Model with all features has an R² of', np.round(r2_score(y, model.fit(X, y).predict(X)), 3)); print('Model with RFE-selected 5 features has an R² of', np.round(r2_rfe, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ value has not changed much even after dropping a feature, so that's good. Typically, we can evaluate $R^2$ iteratively and choose a fair number of predictors to retain by analysing how $R^2$ drops in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_rfe_analysis(X, y):\n",
    "    n_samples = len(X)  # Number of data points\n",
    "    model = LinearRegression()\n",
    "    rfe_records = []  # To store details of each feature drop\n",
    "\n",
    "    # Start with all features and drop one at each iteration\n",
    "    for iteration in range(X.shape[1], 0, -1):\n",
    "        rfe = RFE(estimator = model, n_features_to_select = iteration)\n",
    "        rfe.fit(X, y)\n",
    "        X_selected = X[X.columns[rfe.support_].tolist()]\n",
    "        model.fit(X_selected, y)\n",
    "        r2 = model.score(X_selected, y)\n",
    "        adj_r2 = adjusted_r2(r2, n_samples, X_selected.shape[1])\n",
    "        rfe_records.append({'Iteration': X.shape[1] - iteration + 1, 'Num_Features': X_selected.shape[1],\n",
    "                            'R2': round(r2, 3), 'Adjusted_R2': round(adj_r2, 3)})\n",
    "        \n",
    "    return pd.DataFrame(rfe_records).set_index('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_rfe_analysis(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, both $R^2$ and adjusted $R^2$ decrease with fewer predictors, but the drop in $R^2$ is sharper than that of the adjusted $R^2$ drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the R-squared and adjusted-R-squared change\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(iterative_rfe_analysis(X, y)['Num_Features'], iterative_rfe_analysis(X, y)['R2'], marker = 'o', label = 'R²')\n",
    "plt.plot(iterative_rfe_analysis(X, y)['Num_Features'], iterative_rfe_analysis(X, y)['Adjusted_R2'], marker = 'o', label = 'Adjusted R²')\n",
    "plt.xlabel('Number of Predictors'); plt.ylabel('R² and Adjusted R²'); plt.title('R² and Adjusted R² as a Function of Predictors'); plt.gca().xaxis.set_major_locator(MaxNLocator(integer = True)); plt.gca().invert_xaxis(); plt.legend(); plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While adjusted $R^2$ and $R^2$ follow similar trends, the separation becomes minimal at around four or three predictors. If the $R^2$ for these predictors is acceptable to the business use-case, then it might be a good number of predictors to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Predictors and VIF\n",
    "Categorical variables can significantly distort the variance inflation factor values due to one-hot encoding. When we convert categorical variables into multiple dummy variables, these dummies are inherently correlated because they represent partitions of the same underlying feature. This correlation inflates VIF values, which must be interpreted carefully in the context of multicollinearity assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use seaborn's built-in `'tips'` dataset, which contains information about restaurant bills and tips\n",
    "\n",
    "This dataset includes\n",
    "- **Numeric predictors:** `'total_bill'`, `'size'` (party size)\n",
    "- **Categorical predictors:** `'sex'`, `'smoker'`, `'day'`, `'time'`\n",
    "- **Target variable:** `'tip'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tips dataset\n",
    "tips = sns.load_dataset('tips'); tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips['smoker'].value_counts()  # Distribution of smokers in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's compute VIF values using only the **continuous numerical** predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric_only = tips[['total_bill', 'size']]  # Numeric features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = df.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif(X_numeric_only)  # VIF for numeric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll encode the categorical `'smoker'` variable using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for 'smoker' (drop first category to avoid dummy trap)\n",
    "smoker_dummies = pd.get_dummies(tips['smoker'], prefix = 'smoker', drop_first = True)\n",
    "smoker_dummies.sample(5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The level `'smoker_No'` has been taken as the reference category by default due to alphanumeric reasons. So, a true value here indicates that the person is not a smoker while a false value indicates that they are. The `drop_first` argument drops the first level from a categorical predictor. For example, from the levels $[l_1, l_2, ... , l_n]$, if the levels $[l_2, ... , l_n]$ are all `False`, then $l_1$ will be automatically `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numeric features with categorical dummies\n",
    "X_with_categorical = pd.concat([X_numeric_only, smoker_dummies], axis = 1)\n",
    "X_with_categorical.sample(5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute VIF values for the above set of features. Before we do that, we need to convert the boolean feature into a numerical data type because `variance_inflation_factor()` expects only numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_categorical = X_with_categorical.astype(float)  # Ensure all columns are float\n",
    "calculate_vif(X_with_categorical)  # Calculate VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the VIFs in different cases\n",
    "print('Numerical predictors')\n",
    "for _, row in calculate_vif(X_numeric_only).iterrows(): print(f'  {row['Feature']}: {row['VIF']:.2f}')\n",
    "print('\\nNumerical and categorical predictors')\n",
    "for _, row in calculate_vif(X_with_categorical).iterrows(): print(f'  {row['Feature']}: {row['VIF']:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another categorical variable `'day'` to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips['day'].unique()  # Number of levels in the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding and preparing the feature set\n",
    "day_dummies = pd.get_dummies(tips['day'], prefix = 'day', drop_first = True)\n",
    "X_multiple_categorical = pd.concat([X_numeric_only, smoker_dummies, day_dummies], axis = 1)\n",
    "X_multiple_categorical = X_multiple_categorical.astype(float)\n",
    "X_multiple_categorical.sample(5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have three dummy variables for Friday, Saturday, and Sunday. If all of these are `False` or `0.0`, it means the day is Thursday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_vif(X_multiple_categorical)  # Compute VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the VIFs in different cases\n",
    "print('Numerical predictors')\n",
    "for _, row in calculate_vif(X_numeric_only).iterrows(): print(f'  {row['Feature']}: {row['VIF']:.2f}')\n",
    "print('\\nNumerical and categorical predictors')\n",
    "for _, row in calculate_vif(X_with_categorical).iterrows(): print(f'  {row['Feature']}: {row['VIF']:.2f}')\n",
    "print('\\nNumerical and more categorical predictors')\n",
    "for _, row in calculate_vif(X_multiple_categorical).iterrows(): print(f'  {row['Feature']}: {row['VIF']:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the addition of more categorical levels leads to inflation in the VIFs of numerical features. So, VIF values for categorical dummies should not be over-interpreted in isolation. Categorical predictors introduce structural correlation, which can inflate VIF values. They reflect the encoding structure, not problematic collinearity. The main concern lies with numeric predictors, while categorical variables should be assessed as groups rather than as separate variables, if at all possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "A model can sometimes learn too much from the training data, including random noise. This makes it perform well on the data it has seen but poorly on new data. This is called **overfitting**.\n",
    "\n",
    "For this demonstration, we will create a synthetic dataset in which the target `y` depends on only predictor `x1`, but also add random noise predictors `x2` ... `x20`, that are irrelevant for prediction. This will help us demonstrate how overfitting arises when we include unnecessary predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Set a random seed so that this experiment is replicable\n",
    "X1 = np.random.uniform(-3, 3, 100).reshape(-1, 1)  # True predictor\n",
    "y = 4 * X1 + np.random.randn(100, 1) * 2   # True relation\n",
    "noise = np.random.randn(100, 19)  # Add irrelevant features as noise (19 noisy predictors)\n",
    "X = np.hstack([X1, noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to split our data into training and testing splits for our experiment. We will use `train_test_split()` from `sklearn.model_selection` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)  # Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare our model's performance for only the true feature, and then progressively for $5$, $10$, $15$, and $20$ noisy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train_sub, X_test_sub, y_train, y_test, label):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_sub, y_train)\n",
    "    y_train_pred = model.predict(X_train_sub)\n",
    "    y_test_pred = model.predict(X_test_sub)\n",
    "    model_fit_metrics = {'Model': label, 'Train MSE': round(mean_squared_error(y_train, y_train_pred), 3),\n",
    "                         'Test MSE': round(mean_squared_error(y_test, y_test_pred), 3),\n",
    "                         r'Train $R^2$': round(r2_score(y_train, y_train_pred), 3),\n",
    "                         r'Test $R^2$': round(r2_score(y_test, y_test_pred), 3)}\n",
    "    return model_fit_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive models using a loop\n",
    "results = []\n",
    "for i in range(0, 21, 5):  # 0, 5, 10, 15, 20 noisy features\n",
    "    if i == 0:\n",
    "        desc = 'x1 (true predictor)'\n",
    "        X_train_subset = X_train[:, [0]]\n",
    "        X_test_subset = X_test[:, [0]]\n",
    "    else:\n",
    "        desc = f'x1 + {i} noisy features'\n",
    "        X_train_subset = X_train[:, :i+1]\n",
    "        X_test_subset = X_test[:, :i+1]\n",
    "\n",
    "    results.append(evaluate_model(X_train_subset, X_test_subset, y_train, y_test, desc))\n",
    "    \n",
    "results_df = pd.DataFrame(results); results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how test performance deteriorates as we add more irrelevant features, while training performance improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 3))\n",
    "plt.plot(results_df['Model'], results_df['Train MSE'], marker = 'o', label = 'Training')\n",
    "plt.plot(results_df['Model'], results_df['Test MSE'], marker = 'o', label = 'Testing')\n",
    "plt.xticks(rotation = 30, ha = 'right'); plt.ylabel('MSE'); plt.title('Indications of Overfitting'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add irrelevant predictors, the true predictor (`x1`) should ideally carry the model and the noisy features should have coefficients close to 0, but this isn't always the case due to overfitting. The variance in the irrelevant predictors is used by the model to explain portions of the target variance, hence affecting the true predictor's coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the coefficient magnitude for all predictors\n",
    "# These are comparable because they are on the same scale\n",
    "model_full = LinearRegression()\n",
    "model_full.fit(X_train, y_train)\n",
    "coeffs = pd.DataFrame({'Feature': [f'x{i+1}' for i in range(X_train.shape[1])],  'AbsCoefficient': np.abs(model_full.coef_.flatten())})\n",
    "coeffs_sorted = coeffs.sort_values(by = 'AbsCoefficient', ascending = False)  # Sort by absolute coefficient value\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.bar(coeffs_sorted['Feature'], coeffs_sorted['AbsCoefficient'], color = 'skyblue')\n",
    "plt.xlabel('Feature'); plt.ylabel('Absolute Coefficient Value'); plt.title('Absolute Value of Coefficient for Each Feature'); plt.xticks(rotation = 45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model has mostly ignored the noise, with the coefficients of its features set close to $0$. Still, even small effects can accumulate, so removing noisy features would make the model train faster and perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroscedasticity\n",
    "Residuals or errors indicate how well the model fits the data. Ideally, residuals should scatter randomly around zero with constant variance. Systematic patterns in residuals signal a violation of model assumptions. A common issue is **heteroscedasticity**, when the variance of residuals is not constant across prediction ranges.\n",
    "\n",
    "To demonstrate non-normal errors, we will create a synthetic heteroscedastic data following $y = 3X + \\text{noise} \\times X$, where the error variance is not constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(1, 10, 100).reshape(-1, 1)  # Generate heteroscedastic data\n",
    "noise = np.random.randn(100, 1)  # Generate random noise\n",
    "y = 3 * X + noise * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "residuals = y - y_pred\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print('RMSE:', round(rmse, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 3))\n",
    "plt.subplot(1, 2, 1)  # Raw data with regression line\n",
    "plt.scatter(X, y, alpha = 0.6, color = 'steelblue'); X_line = np.linspace(X.min(), X.max(), 100); y_line = model.predict(X_line.reshape(-1, 1))\n",
    "plt.plot(X_line, y_line, 'r-', linewidth = 2, label = 'Regression Line'); plt.xlabel('X'); plt.ylabel('y'); plt.title('Original Data'); plt.legend(); plt.grid(True, alpha = 0.3)\n",
    "plt.subplot(1, 2, 2)  # Residual plot\n",
    "plt.scatter(y_pred, residuals, alpha = 0.6, color = 'coral'); plt.axhline(y = 0, color = 'black', linestyle = '--', alpha = 0.7); plt.xlabel('Predictions'); plt.ylabel('Residuals'); plt.title('Residual Plot'); plt.grid(True, alpha = 0.3); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plot shows a funnel shaped pattern, indicating heteroscedasticity where error variance is not constant. This can bias standard errors, making model inferences misleading even if the overall fit appears good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the Target Variable\n",
    "We can transform the target variable to stabilise variance in several ways. In this case, we will use the log transformation, as it is good for right-skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y)  # Apply log transformation on target\n",
    "model_log = LinearRegression()  # Model fit on transformed target\n",
    "model_log.fit(X, y_log)\n",
    "y_pred_log = model_log.predict(X)\n",
    "residuals_log = y_log - y_pred_log\n",
    "rmse_log = np.sqrt(mean_squared_error(y_log, y_pred_log))\n",
    "print('RMSE after log-transform:', round(rmse_log, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale of errors has changed, let's compare the $R^2$ score for both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2, rmse = r2_score(y, y_pred), np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'Original has R² = {r2:.3f} and RMSE = {rmse:.3f}')\n",
    "\n",
    "r2_log, rmse_log = r2_score(y_log, y_pred_log), np.sqrt(mean_squared_error(y_log, y_pred_log))\n",
    "print(f'Log-transformed model has R² = {r2_log:.3f} and RMSE = {rmse_log:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the transformation seems to have improved the model performance. Sometimes, a small drop in performance is also acceptable if residuals become more homoscedastic, improving reliability of statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now revisit our residual plots for the original as well as transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 3))\n",
    "plt.subplot(1, 2, 1)  # Raw data with regression line\n",
    "plt.scatter(X, y, alpha = 0.6, color = 'steelblue'); X_line = np.linspace(X.min(), X.max(), 100); y_line = model.predict(X_line.reshape(-1, 1))\n",
    "plt.plot(X_line, y_line, 'r-', linewidth = 2, label = 'Regression Line'); plt.xlabel('X'); plt.ylabel('y'); plt.title('Original Data'); plt.legend(); plt.grid(True, alpha = 0.3)\n",
    "plt.subplot(1, 2, 2)  # Residual plot\n",
    "plt.scatter(y_pred, residuals, alpha = 0.6, color = 'coral'); plt.axhline(y = 0, color = 'black', linestyle = '--', alpha = 0.7); plt.xlabel('Predictions'); plt.ylabel('Residuals'); plt.title('Residual Plot'); plt.grid(True, alpha = 0.3); plt.tight_layout();\n",
    "\n",
    "plt.figure(figsize = (8, 3))\n",
    "plt.subplot(1, 2, 1)  # Raw data with regression line for log-transformed target\n",
    "plt.scatter(X, y_log, alpha = 0.6, color = 'steelblue'); X_line, y_line = np.linspace(X.min(), X.max(), 100), model_log.predict(X_line.reshape(-1, 1))\n",
    "plt.plot(X_line, y_line, 'r-', linewidth = 2, label = 'Regression Line'); plt.xlabel('X'); plt.ylabel('y'); plt.title('Transformed Data'); plt.legend(); plt.grid(True, alpha = 0.3)\n",
    "plt.subplot(1, 2, 2)  # Residual plot\n",
    "plt.scatter(y_pred_log, residuals_log, alpha = 0.6, color = 'coral'); plt.axhline(y = 0, color = 'black', linestyle = '--', alpha = 0.7); plt.xlabel('Predicted Values'); plt.ylabel('Residuals'); plt.title('Residual Plot'); plt.grid(True, alpha = 0.3); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the log-transformed data has less obvious patterns. While this does not fully treat the problem, it is an improvement on the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
